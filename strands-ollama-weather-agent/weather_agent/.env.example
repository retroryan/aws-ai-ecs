# Strands Weather Agent Demo Configuration
# Copy this file to .env and configure your settings

# Model Provider Selection
# Options: "bedrock" (AWS Bedrock) or "ollama" (Local Ollama)
MODEL_PROVIDER=bedrock

# ===== AWS Bedrock Configuration (when MODEL_PROVIDER=bedrock) =====
# Strands works best with these models:
BEDROCK_MODEL_ID=us.anthropic.claude-3-7-sonnet-20250219-v1:0
# Alternative models:
# BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20240620-v1:0
# BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
# BEDROCK_MODEL_ID=meta.llama3-70b-instruct-v1:0

# AWS Bedrock Region
BEDROCK_REGION=us-west-2

# Model Temperature (0-1, lower = more deterministic)
BEDROCK_TEMPERATURE=0

# ===== Ollama Configuration (when MODEL_PROVIDER=ollama) =====
# Ollama model to use (must be pulled first with: ollama pull <model>)
OLLAMA_MODEL=llama3.2
# Alternative models:
# OLLAMA_MODEL=llama3.2:1b
# OLLAMA_MODEL=llama3.1:8b
# OLLAMA_MODEL=mistral
# OLLAMA_MODEL=gemma2:9b

# Ollama host URL
OLLAMA_HOST=http://localhost:11434

# Ollama model parameters
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=4096
OLLAMA_TOP_P=0.9
OLLAMA_TIMEOUT=60

# MCP Server URLs (for local development)
MCP_FORECAST_URL=http://localhost:8081/mcp
MCP_HISTORICAL_URL=http://localhost:8082/mcp
MCP_AGRICULTURAL_URL=http://localhost:8083/mcp

# AWS Credentials (if not using IAM role)
# AWS_ACCESS_KEY_ID=your_aws_access_key
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key

# Logging Configuration
LOG_LEVEL=INFO

# Optional: Enable debug logging for Strands
# STRANDS_DEBUG=true